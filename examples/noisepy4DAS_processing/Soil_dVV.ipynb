{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6f9d3a",
   "metadata": {},
   "source": [
    "# Auto-correlation --> velocity change over 2 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"noisepy4das_repo/NoisePy4DAS-SeaDAS/src\")\n",
    "sys.path.append(\"noisepy4das_repo/NoisePy4DAS-SeaDAS/DASstore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "import DAS_module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import scipy.signal as sgn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from obspy import UTCDateTime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from scipy.signal import butter\n",
    "from scipy.signal import detrend\n",
    "from scipy.signal import decimate\n",
    "from scipy.signal import filtfilt\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.interpolate import interp1d\n",
    "from dasstore.zarr import Client\n",
    "from multiprocessing import Pool\n",
    "from matplotlib import pyplot as plt\n",
    "from das_util import read_decimate, get_tstamp, calc_NFFT\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(wave1, wave2, time, maxshift=0, max_ratio=2):\n",
    "\n",
    "    interp_f = interp1d(time, wave2, bounds_error=False, fill_value=0.)\n",
    "    n1 = np.sum(np.square(wave1))\n",
    "    dt = time[1] - time[0]\n",
    "    cc = 0\n",
    "    relative_ratio = 1\n",
    "    npts = len(time)\n",
    "\n",
    "    for ratio in np.arange(1/max_ratio, max_ratio, 0.01):\n",
    "        dt_new = dt / ratio\n",
    "        time_new = np.arange(time[0], time[-1], dt_new)\n",
    "        wave_new = interp_f(time_new)\n",
    "        \n",
    "        n2 = np.sum(np.square(wave_new))\n",
    "        corr = sgn.correlate(wave1, wave_new) / np.sqrt(n1 * n2)\n",
    "\n",
    "        l_maxshift = min(len(wave_new), maxshift)\n",
    "        r_maxshift = min(len(wave1), maxshift)\n",
    "\n",
    "        st_pt = len(wave_new) - l_maxshift\n",
    "        en_pt = len(wave_new) + r_maxshift+1\n",
    "\n",
    "        cc_best = np.nanmax(corr[st_pt: en_pt])\n",
    "\n",
    "        if cc < cc_best:\n",
    "            cc = cc_best\n",
    "            relative_ratio = ratio\n",
    "\n",
    "    dt_new = dt / relative_ratio\n",
    "    time_new = np.arange(time[0], time[-1], dt_new)\n",
    "    wave_new = interp_f(time_new)\n",
    "    \n",
    "    \n",
    "    return wave_new, np.arange(len(wave_new))*dt, relative_ratio, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_distribution(wave1, wave2, time, maxshift=0, max_ratio=2):\n",
    "\n",
    "    stretch_range = np.arange(1/max_ratio, max_ratio, 0.01)\n",
    "    \n",
    "    interp_f = interp1d(time, wave2, bounds_error=False, fill_value=0.)\n",
    "    n1 = np.sum(np.square(wave1))\n",
    "    dt = time[1] - time[0]\n",
    "    cc = np.zeros(len(stretch_range), dtype = np.float32)\n",
    "    npts = len(time)\n",
    "\n",
    "    for i, ratio in enumerate(stretch_range):\n",
    "        dt_new = dt / ratio\n",
    "        time_new = np.arange(time[0], time[-1], dt_new)\n",
    "        wave_new = interp_f(time_new)\n",
    "        \n",
    "        n2 = np.sum(np.square(wave_new))\n",
    "        corr = sgn.correlate(wave1, wave_new) / np.sqrt(n1 * n2)\n",
    "\n",
    "        l_maxshift = min(len(wave_new), maxshift)\n",
    "        r_maxshift = min(len(wave1), maxshift)\n",
    "\n",
    "        st_pt = len(wave_new) - l_maxshift\n",
    "        en_pt = len(wave_new) + r_maxshift+1\n",
    "\n",
    "        cc[i] = np.nanmax(corr[st_pt: en_pt])\n",
    "    \n",
    "    return stretch_range, cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9961c",
   "metadata": {},
   "source": [
    "## Sort 1-minute files by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/1-fnp/petasaur/p-wd05/harper_plots'\n",
    "file_list = np.array(os.listdir(data_dir))\n",
    "\n",
    "acqu_time = np.array([get_tstamp(i) for i in file_list])\n",
    "new_index = np.argsort(np.array(acqu_time)-acqu_time[0])\n",
    "acqu_time = acqu_time[new_index]\n",
    "\n",
    "file_list = file_list[new_index]\n",
    "file_path = [os.path.join(data_dir,i) for i in file_list]\n",
    "\n",
    "# %% reasonable acquisition time period\n",
    "list_all = np.arange(28,2468)\n",
    "num_sample_all = np.zeros(len(list_all), dtype=np.float64)\n",
    "sample_rate_all = np.zeros(len(list_all), dtype=np.float64)\n",
    "delta_space_all = np.zeros(len(list_all), dtype=np.float64)\n",
    "for i,j in enumerate(list_all):\n",
    "    with h5py.File(file_path[j],'r') as f:      \n",
    "        num_sample_all[i]  = len(f['Acquisition']['Raw[0]']['RawDataTime'][:])\n",
    "        sample_rate_all[i] = f['Acquisition']['Raw[0]'].attrs['OutputDataRate']\n",
    "        delta_space_all[i] = f['Acquisition'].attrs['SpatialSamplingInterval']\n",
    "        \n",
    "# %% exclude files that dropped samples\n",
    "ind_good = np.where(num_sample_all == 120000)[0]\n",
    "list_all = list_all[ind_good]\n",
    "delta_space = delta_space_all[ind_good][0]\n",
    "num_sample = num_sample_all[ind_good][0]\n",
    "sample_rate = sample_rate_all[ind_good][0]\n",
    "delta_time = 1.0 / sample_rate\n",
    "\n",
    "print(f'good acqusition for {len(ind_good)} minutes')\n",
    "\n",
    "del num_sample_all, file_path\n",
    "gc.collect()\n",
    "# %% See if the acquisition time is continuous\n",
    "file_list = file_list[list_all]\n",
    "acqu_time = acqu_time[list_all]\n",
    "file_path = [os.path.join(data_dir,i) for i in file_list]\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(13, 2.5), constrained_layout=True)\n",
    "ax.scatter(list_all, acqu_time.astype('datetime64[m]'), marker='o', s=0.1, edgecolors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16198cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the :30 time stamps in DAS\n",
    "# hourly_index=[37,97,157,217,277,329,389,439,496,556,616,676,736,796,856,916,976,1033,1093,\n",
    "#          1153,1213,1273,1332,1392,1452,1512,1569,1629,1689,1749,1809,1869,1929,1989,\n",
    "#          2048,2108,2168,2228,2288]\n",
    "# locate the :00 time stamps in DAS\n",
    "hourly_index=[67,127,187,247,299,359,419,466,526,586,646,706,766,826,886,946,1003,1063,1123,\n",
    "         1183,1243,1302,1362,1422,1482,1542,1599,1659,1719,1779,1839,1899,1959,2019,\n",
    "         2078,2138,2198,2258,2318]\n",
    "for i in hourly_index:\n",
    "    print(acqu_time[i])\n",
    "\n",
    "csv_file = pd.read_csv('NewportSalop_merged.csv', low_memory=False)\n",
    "j = 32\n",
    "# print(csv_file['Time'][j:j+40])\n",
    "rainfall=csv_file[' Rainfall Total since 0900'][j:j+40].to_numpy()\n",
    "rain_diff=np.diff(rainfall)\n",
    "\n",
    "soil_temp_10=csv_file[' 10cm Soil Temperature'][j+1:j+40].to_numpy()\n",
    "soil_temp_30=csv_file['30cm Soil Temperature'][j+1:j+40].to_numpy()\n",
    "soil_temp_100=csv_file['100cm Soil Temperature'][j+1:j+40].to_numpy()\n",
    "humidity=csv_file['Humidity'][j+1:j+40].to_numpy()\n",
    "print(csv_file['Time'][32:72].to_numpy())\n",
    "len(soil_temp_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bd75e",
   "metadata": {},
   "source": [
    "## Auto correlation --multi-channel, multi-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_file = np.arange(0, len(acqu_time), 5)\n",
    "time_stamps = acqu_time[start_file]\n",
    "\n",
    "samp_freq = 500                # targeted sampling rate\n",
    "freq_norm   = 'no'             # 'no' for no whitening, or 'rma' for running-mean average, 'phase_only' for sign-bit normalization in freq domain.\n",
    "time_norm   = 'one_bit'        # 'no' for no normalization, or 'rma', 'one_bit' for normalization in time domain\n",
    "cc_method   = 'xcorr'          # 'xcorr' for pure cross correlation, 'deconv' for deconvolution; FOR \"COHERENCY\" PLEASE set freq_norm to \"rma\", time_norm to \"no\" and cc_method to \"xcorr\"\n",
    "smooth_N    = 50               # moving window length for time domain normalization if selected (points)\n",
    "smoothspect_N  = 50            # moving window length to smooth spectrum amplitude (points)\n",
    "maxlag      = 5                # lags of cross-correlation to save (sec)\n",
    "\n",
    "# criteria for data selection\n",
    "max_over_std = 10 *9              # threshold to remove window of bad signals: set it to 10*9 if prefer not to remove them\n",
    "num_sample = 30000\n",
    "cc_len = delta_time * num_sample  # correlate length in second\n",
    "step   = delta_time * num_sample  # stepping length in second\n",
    "\n",
    "n_pair = 1\n",
    "n_lag = maxlag * samp_freq * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08d3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######### Loop over frequency\n",
    "# for freqmin in [1,2,3,4,5,6,7,8,9,10,15,20,30,40]:\n",
    "for freqmin in [25]:\n",
    "    freqmax = freqmin * 2\n",
    "    \n",
    "    locations = np.arange(44,95,1)\n",
    "    corr_all_time = np.zeros((len(locations), len(start_file), n_lag, n_pair), dtype = np.float32)\n",
    "    \n",
    "    ######### Loop over channel\n",
    "    for loc, auto_ch in enumerate(locations):\n",
    "\n",
    "        # start and end channel index for the sub-array\n",
    "        cha1 = auto_ch\n",
    "        cha2 = auto_ch + 1\n",
    "        cha_list = np.array(range(cha1, cha2)) \n",
    "        nsta = len(cha_list)\n",
    "\n",
    "        prepro_para = {'freqmin':freqmin,\n",
    "                       'freqmax':freqmax,\n",
    "                       'sps':sample_rate,\n",
    "                       'npts_chunk':cc_len*sample_rate,\n",
    "                       'nsta':nsta,\n",
    "                       'cha_list':cha_list,\n",
    "                       'samp_freq':samp_freq,\n",
    "                       'freq_norm':freq_norm,\n",
    "                       'time_norm':time_norm,\n",
    "                       'cc_method':cc_method,\n",
    "                       'smooth_N':smooth_N,\n",
    "                       'smoothspect_N':smoothspect_N,\n",
    "                       'maxlag':maxlag,\n",
    "                       'max_over_std':max_over_std}\n",
    "        \n",
    "        ######### Loop over time windows\n",
    "        for i,iquake in enumerate(tqdm(start_file)):\n",
    "            corr_full = np.zeros((n_lag, n_pair), dtype = np.float32)\n",
    "            stack_full = np.zeros((1, n_pair), dtype = np.int32)\n",
    "            \n",
    "            for j in [0, 15, 30, 45]:\n",
    "                st_samp = int(j * samp_freq)\n",
    "            \n",
    "                with h5py.File(file_path[iquake],'r') as f:\n",
    "                    minute_data = f['Acquisition']['Raw[0]']['RawData'][st_samp:st_samp+num_sample, cha1:cha2]\n",
    "\n",
    "                # perform pre-processing\n",
    "                trace_stdS, dataS = DAS_module.preprocess_raw_make_stat(minute_data, prepro_para)\n",
    "\n",
    "                # do normalization if needed\n",
    "                white_spect = DAS_module.noise_processing(dataS, prepro_para)\n",
    "                Nfft = white_spect.shape[1]\n",
    "                data = white_spect[:, :(Nfft // 2)]\n",
    "\n",
    "                del dataS, white_spect\n",
    "                gc.collect()\n",
    "\n",
    "                ind = np.where((trace_stdS < prepro_para['max_over_std']) &\n",
    "                               (trace_stdS > 0) &\n",
    "                      (np.isnan(trace_stdS) == 0))[0]\n",
    "                if not len(ind):\n",
    "                    raise ValueError('no data- reduce max_over_std')\n",
    "                sta = cha_list[ind]\n",
    "                white_spect = data[ind]\n",
    "\n",
    "                for iiS in range(len(sta)):\n",
    "                    # smooth the source spectrum\n",
    "                    sfft1 = DAS_module.smooth_source_spect(white_spect[iiS], prepro_para)\n",
    "                    # Xcorr\n",
    "                    corr, tindx = DAS_module.correlate(sfft1, white_spect[iiS:], prepro_para, Nfft)\n",
    "                    tsta = sta[iiS:]\n",
    "                    receiver_lst = tsta[tindx]\n",
    "                    iS = int((cha2*2 - cha1 - sta[iiS] + 1) * (sta[iiS] - cha1) / 2)\n",
    "\n",
    "                    # sub-stack\n",
    "                    corr_full[:, iS + receiver_lst - sta[iiS]] += corr.T / np.var(sfft1)/len(corr)\n",
    "                    stack_full[:, iS + receiver_lst - sta[iiS]] += 1\n",
    "\n",
    "            corr_full /= stack_full\n",
    "\n",
    "            corr_all_time[loc, i,:,:] = corr_full\n",
    "\n",
    "        plt.figure(figsize = (12, 4), dpi = 200)\n",
    "        data_plot=corr_all_time[loc, :, 2500:2650, 0].T\n",
    "        x=np.arange(data_plot.shape[1])\n",
    "        y=np.arange(data_plot.shape[0])\n",
    "        xts=np.array(time_stamps)\n",
    "        xhrs=np.round((xts - xts[0]).astype('float')/3600, 2)\n",
    "\n",
    "        plt.pcolormesh(x, y, data_plot, shading='auto', vmin=-0.4, vmax=0.4, cmap = 'RdBu')\n",
    "\n",
    "        plt.yticks(np.arange(0, data_plot.shape[0], 25), \n",
    "                   (np.arange(0, data_plot.shape[0], 25))/samp_freq, fontsize = 12)\n",
    "        plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "\n",
    "        plt.xlabel(\"ACF time (hour)\", fontsize = 16)\n",
    "        plt.ylabel(\"Time lag (sec)\", fontsize = 16)\n",
    "        plt.title('Channel '+str(auto_ch)+ '_' + str(freqmin)+'_'+str(freqmax)+'Hz', fontsize = 20)\n",
    "        bar = plt.colorbar()\n",
    "        bar.set_label('Auto-correlation Coefficient', fontsize = 15)\n",
    "        \n",
    "    # Save all ACFs\n",
    "    with h5py.File('/fd1/QibinShi_data/England_farm/autocorr_' + str(freqmin) + '_' + str(freqmax) + 'Hz.hdf5', 'w') as f:\n",
    "        f.create_dataset(\"autocorr\", data=corr_all_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del corr_all_time\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223e9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "win_st = int(0.012*samp_freq)\n",
    "\n",
    "with h5py.File('/fd1/QibinShi_data/England_farm/autocorr_25_50Hz.hdf5', 'r') as f:\n",
    "    corr_all_time = f['autocorr'][:, :, 2500:2650, 0]\n",
    "    \n",
    "for iloc in range(51):\n",
    "    data_plot=corr_all_time[iloc].T\n",
    "    x=np.arange(data_plot.shape[1])\n",
    "    y=np.arange(data_plot.shape[0])\n",
    "    xts=np.array(time_stamps)\n",
    "    xhrs=np.round((xts - xts[0]).astype('float')/3600, 2)\n",
    "    plt.figure(figsize = (10, 3), dpi = 200)\n",
    "    for i in x[::5]:\n",
    "        plt.plot(y/samp_freq, data_plot[:,i])\n",
    "\n",
    "    plt.plot(y/samp_freq, np.mean(data_plot, axis=1), c='k', lw=9, alpha=0.3, label='mean of all')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('channel '+ str(iloc+44))\n",
    "    plt.xlabel('lag time (s)')\n",
    "    plt.axvline(x=win_st/samp_freq, color='k', linestyle='--', label='1st reflection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a746e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = y[win_st:] / samp_freq\n",
    "stack_stretch=np.zeros((51, len(time)), dtype = np.float32)\n",
    "for iloc in range(51):\n",
    "    data_plot=corr_all_time[iloc].T\n",
    "    x=np.arange(data_plot.shape[1])\n",
    "    y=np.arange(data_plot.shape[0])\n",
    "    trunc_acf = data_plot[win_st:, :]\n",
    "    trunc_stk = np.mean(trunc_acf, axis=1)\n",
    "    \n",
    "    count = 0\n",
    "    plt.figure(figsize = (10, 3), dpi = 200)\n",
    "    for i in x[::5]:\n",
    "        tmp=np.zeros_like(time)\n",
    "        stretched, time_new, ratio, cc = stretch(trunc_stk, trunc_acf[:,i], time, max_ratio=2)\n",
    "        if cc > 0.5:\n",
    "            length=min(len(stretched), len(time))\n",
    "            tmp[:length]=stretched[:length]\n",
    "            stack_stretch[iloc, :] += tmp\n",
    "            plt.plot(time, tmp)\n",
    "            count +=1\n",
    "        else:continue\n",
    "    plt.plot(time, stack_stretch[iloc, :]/count, c='k', lw=9, alpha=0.5, label='mean of all')\n",
    "    plt.title('channel '+ str(iloc+44)); plt.legend(); plt.xlabel('lag time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c07f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_ratio = np.zeros((51, len(x)), dtype = np.float32)\n",
    "all_cc = np.zeros((51, len(x)), dtype = np.float32)\n",
    "freqmin = 25\n",
    "freqmax = freqmin*2\n",
    "for iloc in range(51):\n",
    "    data_plot=corr_all_time[iloc].T\n",
    "    trunc_acf = data_plot[win_st:, :]\n",
    "    trunc_stk = stack_stretch[iloc, :]/count\n",
    "    strecthed_acfs = np.zeros_like(trunc_acf)\n",
    "    for i in x:\n",
    "        stretched, time_new, ratio, cc = stretch(trunc_stk, trunc_acf[:,i], time, max_ratio=1.5)\n",
    "        length=min(len(stretched), len(time))\n",
    "        strecthed_acfs[:length, i] = stretched[:length]\n",
    "\n",
    "        all_ratio[iloc, i] = ratio\n",
    "        all_cc[iloc, i] = cc\n",
    "    \n",
    "    plt.figure(figsize = (12, 5), dpi = 200)\n",
    "    plt.pcolormesh(x, time, strecthed_acfs, shading='auto', vmin=-0.2, vmax=0.2, cmap = 'RdBu')\n",
    "\n",
    "    plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "\n",
    "    plt.xlabel(\"ACF time (hour)\", fontsize = 16)\n",
    "    plt.ylabel(\"Time lag (sec)\", fontsize = 16)\n",
    "    plt.title('channel '+ str(iloc+44)+ '_' + str(freqmin)+'_'+str(freqmax)+'Hz', fontsize = 20)\n",
    "    bar = plt.colorbar()\n",
    "    bar.set_label('Auto-correlation Coefficient', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cb201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save all stretch ratio and CC\n",
    "with h5py.File('dvv_stretch_cc_' + str(freqmin) + '_' + str(freqmax) + 'Hz.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"ratio\", data=all_ratio)\n",
    "    f.create_dataset(\"cc\", data=all_cc)\n",
    "    \n",
    "for iloc in range(51):\n",
    "    plt.figure(figsize = (10, 2), dpi = 200)\n",
    "\n",
    "    plt.scatter(x, all_ratio[iloc], cmap='viridis', c=np.array(all_cc[iloc]), s=20, marker='o')\n",
    "    plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('time (hour)')\n",
    "    plt.ylabel('stretch ratio')\n",
    "    plt.title('channel '+ str(iloc+44)+ '_' + str(freqmin)+'_'+str(freqmax)+'Hz', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd735f2",
   "metadata": {},
   "source": [
    "## Calculate PSD for each 1-min file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d00775",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_path[200],'r') as f:\n",
    "    minute_data = f['Acquisition']['Raw[0]']['RawData'][:, 74]\n",
    "    sample_rate = f['Acquisition']['Raw[0]'].attrs['OutputDataRate']\n",
    "    dsamp_data = decimate(minute_data, q=5, ftype='fir', zero_phase=True)\n",
    "freq, psd = sgn.welch(dsamp_data, sample_rate/5)\n",
    "freq1,psd1=sgn.periodogram(dsamp_data, sample_rate/5)\n",
    "freq2, psd2=sgn.csd(dsamp_data,dsamp_data,sample_rate/5)\n",
    "\n",
    "plt.semilogy(freq1, psd1)\n",
    "# plt.xlim(0,200)\n",
    "plt.title('Power Spectral Density')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "\n",
    "plt.semilogy(freq2, psd2)\n",
    "# plt.xlim(0,200)\n",
    "plt.title('Power Spectral Density')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power/Frequency (dB/Hz)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d931d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_file = np.arange(0, len(acqu_time), 1)\n",
    "time_stamps = acqu_time[start_file]\n",
    "    \n",
    "locations = np.arange(44,46,2)\n",
    "PSD_all_time = np.zeros((len(locations), len(start_file), len(psd)), dtype = np.float32)\n",
    "\n",
    "for loc, ch in enumerate(locations):\n",
    "    for i,iquake in enumerate(tqdm(start_file)):\n",
    "        with h5py.File(file_path[iquake],'r') as f:\n",
    "            minute_data = f['Acquisition']['Raw[0]']['RawData'][:, ch]\n",
    "            dsamp_data = decimate(minute_data, q=5, ftype='fir', zero_phase=True)\n",
    "        freq, psd = sgn.welch(dsamp_data, sample_rate/5)\n",
    "        PSD_all_time[loc, i,:] = psd\n",
    "    plt.figure(figsize = (12, 4), dpi = 200)\n",
    "    data_plot=PSD_all_time[loc, :, :].T\n",
    "\n",
    "    plt.pcolormesh(np.arange(2408), freq, data_plot, shading='auto', vmin=0, vmax=0.001, cmap = 'viridis')\n",
    "#     plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "    plt.xlabel(\"ACF time (hour)\", fontsize = 16)\n",
    "    plt.ylabel(\"Frequency (Hz)\", fontsize = 16)\n",
    "    plt.title('Channel '+str(ch), fontsize = 20)\n",
    "    plt.ylim(0,200)\n",
    "    bar = plt.colorbar()\n",
    "    bar.set_label('Power Spectral Density', fontsize = 15)\n",
    "    # Save all ACFs\n",
    "    with h5py.File('/fd1/QibinShi_data/England_farm/psd_ch_' + str(ch) + '.hdf5', 'w') as f:\n",
    "        f.create_dataset(\"psd_all_time\", data=PSD_all_time)\n",
    "        f.create_dataset(\"frequency\", data=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_psd=np.sum(PSD_all_time[:, :, 52:90], axis=-1)\n",
    "ave_psd=np.mean(time_psd, axis=0)\n",
    "\n",
    "for loc, ch in enumerate(locations):\n",
    "    plt.figure(figsize = (12, 4), dpi = 200)\n",
    "    \n",
    "    plt.plot(np.arange(time_psd[loc].shape[-1]), time_psd[loc])\n",
    "#     plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "\n",
    "    plt.xlabel(\"ACF time (hour)\", fontsize = 16)\n",
    "    plt.ylabel(\"Sum of PSD\", fontsize = 16)\n",
    "    plt.title('Channel '+str(ch), fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## smoothing over 1 hr window\n",
    "hr_psd=[]\n",
    "for ind in hourly_index:\n",
    "    st_ind = int(ind - 60)\n",
    "    ed_ind = st_ind + 60\n",
    "    psd_int=np.sum(ave_psd[st_ind:ed_ind])\n",
    "    \n",
    "    hr_psd.append(psd_int)\n",
    "\n",
    "plt.figure(figsize = (12, 4), dpi = 200)   \n",
    "plt.scatter(hourly_index, rain_diff/2.3, label='1 hr rain', color='g', marker='o', s=100)\n",
    "plt.scatter(hourly_index, np.array(hr_psd), label='1 hr psd', color='b', marker='*', s=100)\n",
    "plt.plot(hourly_index, np.array(hr_psd))\n",
    "plt.plot(np.arange(ave_psd.shape[-1]), ave_psd*5, label='psd')\n",
    "# plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "\n",
    "plt.xlabel(\"ACF time (min)\", fontsize = 16)\n",
    "plt.ylabel(\"Sum of PSD\", fontsize = 16)\n",
    "plt.title('Channel-wise average, hourly stack', fontsize = 20)  \n",
    "plt.ylim(-0.2, 2)\n",
    "plt.legend()\n",
    "\n",
    "np.argmax(rain_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch again to get distribution\n",
    "stretch_range = np.arange(1/2, 2, 0.01)\n",
    "all_ratio = np.zeros((51, len(stretch_range), len(x)), dtype = np.float32)\n",
    "all_cc = np.zeros((51, len(stretch_range), len(x)), dtype = np.float32)\n",
    "for iloc in [18, 30]:\n",
    "    data_plot=corr_all_time[iloc].T\n",
    "    trunc_acf = data_plot[win_st:, :]\n",
    "    trunc_stk = stack_stretch[iloc, :]/count\n",
    "    strecthed_acfs = np.zeros_like(trunc_acf)\n",
    "    for i in x:\n",
    "        ratios, ccs = stretch_distribution(trunc_stk, trunc_acf[:,i], time, max_ratio=2)\n",
    "        all_cc[iloc, :, i] = ccs  \n",
    "    plt.figure(figsize = (12, 3), dpi = 100)\n",
    "    plt.pcolormesh(x, stretch_range, all_cc[iloc], shading='auto', vmin=0.2, vmax=1, cmap = 'hot')\n",
    "    plt.xticks(np.arange(0, data_plot.shape[1],100), xhrs[:data_plot.shape[1]:100], fontsize = 12)\n",
    "    plt.xlabel(\"ACF time (hour)\", fontsize = 16)\n",
    "    plt.ylabel(\"stretch ratio\", fontsize = 16, color='r')\n",
    "    plt.ylim(0.5, 1.5)\n",
    "    plt.title('channel '+ str(iloc+44)+ '_' + str(freqmin)+'_'+str(freqmax)+'Hz', fontsize = 20)\n",
    "    bar = plt.colorbar()\n",
    "    bar.set_label('Cross-correlation Coefficient', fontsize = 15)\n",
    "    \n",
    "    axcopy = plt.twinx()\n",
    "    axcopy.scatter(np.array(hourly_index)/5, soil_temp_10-7.5, label='T_10cm', color='w', marker='^')\n",
    "    axcopy.scatter(np.array(hourly_index)/5, (soil_temp_30-7.5), label='T_30cm', color='w', marker='s')\n",
    "    axcopy.scatter(np.array(hourly_index)/5, (soil_temp_100-7.5), label='T_100cm', color='w', marker='v')\n",
    "    axcopy.scatter(np.array(hourly_index)/5, rain_diff*1.2, label='1hr rain', color='g', marker='o', s=100)\n",
    "    axcopy.scatter(np.array(hourly_index)/5, 4-(humidity-63)/12, label='dryness', color='g', marker='+', s=100)\n",
    "#     axcopy.plot(x, ave_psd*10, label='sum_psd')\n",
    "#     axcopy.scatter(np.array(hourly_index)/5, np.array(hr_psd)*8, label='hr_psd', color='b', marker='d', s=100)\n",
    "    axcopy.legend(fontsize = 7)\n",
    "    axcopy.set_ylim(-0.2,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
