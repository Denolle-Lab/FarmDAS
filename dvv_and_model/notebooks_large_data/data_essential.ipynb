{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Produce Small Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from das_util import compute_misfit, get_tstamp\n",
    "from scipy.ndimage import gaussian_filter1d, gaussian_filter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "data_dir='../../data_farm/data/'\n",
    "save_dir='./data/'\n",
    "\n",
    "psd_data = os.path.join(data_dir, 'psd_all.hdf5')\n",
    "acf_data = os.path.join(data_dir, 'autocorr_15_60Hz.hdf5')\n",
    "acf_data_stretched = os.path.join(data_dir, 'autocorr_15_60Hz_stretched.hdf5')\n",
    "acf_data_stretchedx2 = os.path.join(data_dir, 'autocorr_15_60Hz_stretchedx2.hdf5')\n",
    "acf_data_stretchedx3 = os.path.join(data_dir, 'autocorr_15_60Hz_stretchedx3.hdf5')\n",
    "harper_met = os.path.join(data_dir, 'NewportSalop_merged.csv')\n",
    "reg_met = os.path.join(data_dir, 'met_newport.csv')\n",
    "spatial_data = os.path.join(data_dir, 'interp_dv_tillage_tire.csv')\n",
    "phy_data = os.path.join(data_dir, 'Soil_phy.csv')\n",
    "phy_data_interpolate = os.path.join(data_dir, 'interpolated_soil_phy.csv')\n",
    "\n",
    "freqmin=25\n",
    "freqmax=50\n",
    "samp_freq = 500\n",
    "dchan = 3.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACFs (H5)\n",
    "Raw, strected, stretchedx2, stretchedx3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read big h5 files\n",
    "with h5py.File(acf_data, 'r') as f:\n",
    "    corr_all_time_ch18 = f['autocorr'][18, :, 2500:3000, 0]\n",
    "    corr_all_time_ch33 = f['autocorr'][33, :, 2500:3000, 0]\n",
    "    corr_all_time_ch44 = f['autocorr'][44, :, 2500:3000, 0]\n",
    "    corr_all_channel_30_40min = f['autocorr'][:50, 30:40, 2500:2700, 0]\n",
    "with h5py.File(acf_data_stretched, 'r') as f:\n",
    "    corr_all_time_stretched1_ch18 = f['autocorr'][18, :, :500]\n",
    "    corr_all_time_stretched1_ch33 = f['autocorr'][33, :, :500]\n",
    "    corr_all_time_stretched1_ch44 = f['autocorr'][44, :, :500]\n",
    "with h5py.File(acf_data_stretchedx2, 'r') as f:\n",
    "    corr_all_time_stretched2_ch18 = f['autocorr'][18, :, :500]\n",
    "    corr_all_time_stretched2_ch33 = f['autocorr'][33, :, :500]\n",
    "    corr_all_time_stretched2_ch44 = f['autocorr'][44, :, :500]\n",
    "with h5py.File(acf_data_stretchedx3, 'r') as f:\n",
    "    corr_all_time_stretched3_ch18 = f['autocorr'][18, :, :500]\n",
    "    corr_all_time_stretched3_ch33 = f['autocorr'][33, :, :500]\n",
    "    corr_all_time_stretched3_ch44 = f['autocorr'][44, :, :500]\n",
    "\n",
    "### Save small h5 files\n",
    "with h5py.File(os.path.join(save_dir, 'autocorr_15_60Hz_3chs500pts.hdf5'), 'w') as f:\n",
    "    f.create_dataset('corr_all_time_ch18', data=corr_all_time_ch18)\n",
    "    f.create_dataset('corr_all_time_ch33', data=corr_all_time_ch33)\n",
    "    f.create_dataset('corr_all_time_ch44', data=corr_all_time_ch44)\n",
    "    f.create_dataset('corr_all_time_stretched1_ch18', data=corr_all_time_stretched1_ch18)\n",
    "    f.create_dataset('corr_all_time_stretched1_ch33', data=corr_all_time_stretched1_ch33)\n",
    "    f.create_dataset('corr_all_time_stretched1_ch44', data=corr_all_time_stretched1_ch44)\n",
    "    f.create_dataset('corr_all_time_stretched2_ch18', data=corr_all_time_stretched2_ch18)\n",
    "    f.create_dataset('corr_all_time_stretched2_ch33', data=corr_all_time_stretched2_ch33)\n",
    "    f.create_dataset('corr_all_time_stretched2_ch44', data=corr_all_time_stretched2_ch44)\n",
    "    f.create_dataset('corr_all_time_stretched3_ch18', data=corr_all_time_stretched3_ch18)\n",
    "    f.create_dataset('corr_all_time_stretched3_ch33', data=corr_all_time_stretched3_ch33)\n",
    "    f.create_dataset('corr_all_time_stretched3_ch44', data=corr_all_time_stretched3_ch44)\n",
    "\n",
    "with h5py.File(os.path.join(save_dir, 'autocorr_15_60Hz_30_40min.hdf5'), 'w') as f:\n",
    "    f.create_dataset('corr_all_channel_30_40min', data=corr_all_channel_30_40min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dVV (H5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iteration 1\n",
    "with h5py.File(os.path.join(data_dir, 'final_peaks_interp_smooth_15_60Hz.h5'), 'r') as f:\n",
    "    stretch_ratio = f['final_peaks'][:]\n",
    "\n",
    "## de ratio using the humidist time period\n",
    "deratio_dvv = stretch_ratio / np.repeat(np.mean(stretch_ratio[:, 269:280], axis=1)[:, np.newaxis], 482, axis=1) - 1\n",
    "\n",
    "with h5py.File(os.path.join(save_dir, 'final_peaks_deRatio.h5'), 'w') as f:\n",
    "    f.create_dataset('deratio_dvv', data=deratio_dvv)\n",
    "\n",
    "\n",
    "### Iteration 2\n",
    "with h5py.File(os.path.join(data_dir, 'final_peaks_15_60Hz_2nd_iteration.h5'), 'r') as f:\n",
    "    stretch_ratio = f['final_peaks'][:50]\n",
    "stretch_ratio[stretch_ratio<0.75] = 0.75\n",
    "\n",
    "## de ratio using the humidist time period\n",
    "stretch_ratio = stretch_ratio / np.repeat(np.mean(stretch_ratio[:, 269:280], axis=1)[:, np.newaxis], 482, axis=1)\n",
    "\n",
    "stretch_ratio[np.isnan(stretch_ratio)] = 1\n",
    "\n",
    "deratio_dvv = (deratio_dvv + 1) * stretch_ratio - 1\n",
    "\n",
    "with h5py.File(os.path.join(save_dir, 'final_peaks_deRatio_2iterations.h5'), 'w') as f:\n",
    "    f.create_dataset('deratio_dvv', data=deratio_dvv)\n",
    "\n",
    "\n",
    "### Iteration 3\n",
    "with h5py.File(os.path.join(data_dir, 'final_peaks_15_60Hz_3rd_iteration.h5'), 'r') as f:\n",
    "    stretch_ratio = f['final_peaks'][:50]-0.1\n",
    "\n",
    "## de ratio using the humidist time period\n",
    "stretch_ratio = stretch_ratio / np.repeat(np.mean(stretch_ratio[:, 269:280], axis=1)[:, np.newaxis], 482, axis=1)\n",
    "\n",
    "stretch_ratio[np.isnan(stretch_ratio)] = 1\n",
    "\n",
    "deratio_dvv = (deratio_dvv + 1) * stretch_ratio - 1\n",
    "\n",
    "with h5py.File(os.path.join(save_dir, 'final_peaks_deRatio_3iterations.h5'), 'w') as f:\n",
    "    f.create_dataset('deratio_dvv', data=deratio_dvv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretched ACFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ratio = deratio_dvv + 1\n",
    "\n",
    "# shape_3d = (corr_all_time.shape[0], corr_all_time.shape[1], int(corr_all_time.shape[2]*all_ratio.max())+1)\n",
    "# corr_all_time_stretched =  np.zeros(shape_3d, dtype=np.float32)\n",
    "\n",
    "# for iloc in range(50):\n",
    "#     for itime in range(482):\n",
    "#         ratio = all_ratio[iloc, itime]\n",
    "#         tmp = zoom(corr_all_time[iloc, itime], ratio, order=1)\n",
    "#         corr_all_time_stretched[iloc, itime, :len(tmp)] = tmp \n",
    "    \n",
    "    # plt.figure(figsize = (12, 5))\n",
    "    # plt.imshow(corr_all_time_stretched[iloc, :, :500].T, aspect='auto', vmin=-0.2, vmax=0.2, cmap='RdBu', origin='lower')\n",
    "\n",
    "    # plt.xlabel(\"ACF time (x 5 minutes)\", fontsize = 16)\n",
    "    # plt.ylabel(\"Time lag (sec)\", fontsize = 16)\n",
    "    # bar = plt.colorbar()\n",
    "    # bar.set_label('Auto-correlation Coefficient', fontsize = 15)\n",
    "\n",
    "### stack strected ACF for a static GF\n",
    "# mean_acf = np.mean(corr_all_time_stretched, axis=1)\n",
    "\n",
    "\n",
    "# ### Save the stretched ACF\n",
    "# with h5py.File('data/autocorr_15_60Hz_stretchedx3.hdf5', 'w') as f:\n",
    "#     f.create_dataset('autocorr', data=corr_all_time_stretched)\n",
    "#     f.create_dataset('mean_acf', data=mean_acf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PSD of high frequency DAS\n",
    "with h5py.File(psd_data, 'r') as f:\n",
    "    freq = f['frequency'][:]\n",
    "    file_list = f['daslist'][:]\n",
    "    PSD_all_time = f['psd_all_time'][:]\n",
    "    \n",
    "acqu_time = np.array([get_tstamp(i) for i in file_list.astype('U')])\n",
    "\n",
    "## Index on the hour\n",
    "hourly_index = [67,127,187,247,299,359,419,466,526,586,646,706,766,826,886,946,1003,1063,1123,\n",
    "         1183,1243,1302,1362,1422,1482,1542,1599,1659,1719,1779,1839,1899,1959,2019,\n",
    "         2078,2138,2198,2258,2318]\n",
    "\n",
    "## Mean PSD over channels\n",
    "ave_psd=np.mean(np.sum(PSD_all_time[0:44, :, 52:90], axis=-1) * (freq[1]-freq[0]), axis=0) - 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_PSD=np.sum(PSD_all_time[:, :, 52:90], axis=-1) * (freq[1]-freq[0])* 3\n",
    "int_PSD=gaussian_filter1d(int_PSD, 4, axis=0) - 0.003\n",
    "\n",
    "### Save small h5\n",
    "with h5py.File(os.path.join(save_dir, 'integrated_psd.hdf5'), 'w') as f:\n",
    "    f.create_dataset('int_PSD', data=int_PSD)\n",
    "    f.create_dataset('freq', data=freq)\n",
    "    f.create_dataset('file_list', data=file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil physical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/_f91r5t1277f16yl_03mnxkw0000gn/T/ipykernel_59321/2840630756.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phy_1['ind'] = 36 - phy_1['Plot']\n",
      "/var/folders/94/_f91r5t1277f16yl_03mnxkw0000gn/T/ipykernel_59321/2840630756.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phy_2['ind'] = 33 - phy_2['Plot']\n",
      "/var/folders/94/_f91r5t1277f16yl_03mnxkw0000gn/T/ipykernel_59321/2840630756.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phy_3['ind'] = 30 - phy_3['Plot']\n"
     ]
    }
   ],
   "source": [
    "### Read\n",
    "phy_paras = pd.read_csv(os.path.join(data_dir, 'Soil_phy.csv'))\n",
    "\n",
    "phy_1 = phy_paras.loc[phy_paras['Plot'].between(1,9)]\n",
    "phy_2 = phy_paras.loc[phy_paras['Plot'].between(10,18)]\n",
    "phy_3 = phy_paras.loc[phy_paras['Plot'].between(19,27)]\n",
    "\n",
    "phy_1['ind'] = 36 - phy_1['Plot'] \n",
    "phy_2['ind'] = 33 - phy_2['Plot']\n",
    "phy_3['ind'] = 30 - phy_3['Plot']\n",
    "\n",
    "phy_all = pd.concat([phy_1, phy_2, phy_3])\n",
    "phy_10cm = phy_all.loc[phy_all['Depth (cm)'] == 10]\n",
    "phy_20cm = phy_all.loc[phy_all['Depth (cm)'] == 20]\n",
    "phy_30cm = phy_all.loc[phy_all['Depth (cm)'] == 30]\n",
    "\n",
    "\n",
    "#### Interpolate porosity and bulk density\n",
    "f = interp1d(np.array(phy_30cm['ind'])*4-3.3, np.array(phy_30cm['Porosity (%)']), bounds_error=False, fill_value=48)\n",
    "interp_porosity_30cm = f(np.arange(50)*3.19)\n",
    "\n",
    "f = interp1d(np.array(phy_20cm['ind'])*4-3.3, np.array(phy_20cm['Porosity (%)']), bounds_error=False, fill_value=48)\n",
    "interp_porosity_20cm = f(np.arange(50)*3.19)\n",
    "\n",
    "f = interp1d(np.array(phy_10cm['ind'])*4-3.3, np.array(phy_10cm['Porosity (%)']), bounds_error=False, fill_value=48)\n",
    "interp_porosity_10cm = f(np.arange(50)*3.19)\n",
    "\n",
    "f = interp1d(np.array(phy_30cm['ind'])*4-3.3, np.array(phy_30cm['Dry Bulk density (g/cm3)']), bounds_error=False, fill_value=1.35)\n",
    "interp_bulk_density_30cm = f(np.arange(50)*3.19)\n",
    "\n",
    "f = interp1d(np.array(phy_20cm['ind'])*4-3.3, np.array(phy_20cm['Dry Bulk density (g/cm3)']), bounds_error=False, fill_value=1.35)\n",
    "interp_bulk_density_20cm = f(np.arange(50)*3.19)\n",
    "\n",
    "f = interp1d(np.array(phy_10cm['ind'])*4-3.3, np.array(phy_10cm['Dry Bulk density (g/cm3)']), bounds_error=False, fill_value=1.35)   \n",
    "interp_bulk_density_10cm = f(np.arange(50)*3.19)\n",
    "\n",
    "interp_data = pd.DataFrame({\n",
    "    'distance (m)': np.arange(50) * 3.19,\n",
    "    'interp_porosity_10cm': interp_porosity_10cm,\n",
    "    'interp_porosity_20cm': interp_porosity_20cm,\n",
    "    'interp_porosity_30cm': interp_porosity_30cm,\n",
    "    'interp_bulk_density_10cm': interp_bulk_density_10cm,\n",
    "    'interp_bulk_density_20cm': interp_bulk_density_20cm,\n",
    "    'interp_bulk_density_30cm': interp_bulk_density_30cm\n",
    "})\n",
    "### Save\n",
    "interp_data.to_csv(os.path.join(save_dir, 'interpolated_soil_phy.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quakeflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
