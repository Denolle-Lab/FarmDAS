{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4b25c3",
   "metadata": {},
   "source": [
    "# Farm DAS correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40282f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"noisepy4das_repo/NoisePy4DAS-SeaDAS/src\")\n",
    "sys.path.append(\"noisepy4das_repo/NoisePy4DAS-SeaDAS/DASstore\")\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "import DAS_module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from obspy import UTCDateTime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from scipy.signal import butter\n",
    "from scipy.signal import detrend\n",
    "from scipy.signal import decimate\n",
    "from scipy.signal import filtfilt\n",
    "from scipy.signal import spectrogram\n",
    "from dasstore.zarr import Client\n",
    "from multiprocessing import Pool\n",
    "from matplotlib import pyplot as plt\n",
    "from das_util import read_decimate, get_tstamp, calc_NFFT\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804b338",
   "metadata": {},
   "source": [
    "# QC on metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefba900",
   "metadata": {},
   "source": [
    "### Sort files by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82328529",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/1-fnp/petasaur/p-wd05/harper_plots'\n",
    "file_list = np.array(os.listdir(data_dir))\n",
    "acqu_time = np.array([get_tstamp(i) for i in file_list])\n",
    "\n",
    "new_index = np.argsort(np.array(acqu_time)-acqu_time[0])\n",
    "\n",
    "file_list = file_list[new_index]\n",
    "acqu_time = acqu_time[new_index]\n",
    "file_path = [os.path.join(data_dir,i) for i in file_list]\n",
    "print(file_list[:5])\n",
    "print('Total number of files:', len(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ba3e0",
   "metadata": {},
   "source": [
    "### time and space sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% two quiet time period are manually found\n",
    "list1 = np.arange(500,600)\n",
    "list2 = np.arange(1900,2000)\n",
    "list_all = np.concatenate((list1, list2))\n",
    "\n",
    "# %% reasonable acquisition time period\n",
    "list_all = np.arange(28,2468)\n",
    "\n",
    "num_file = len(list_all)\n",
    "gauge_length_all = np.zeros(num_file, dtype=np.float64)\n",
    "delta_space_all = np.zeros(num_file, dtype=np.float64)\n",
    "num_channel_all = np.zeros(num_file, dtype=np.float64)\n",
    "sample_rate_all = np.zeros(num_file, dtype=np.float64)\n",
    "num_sample_all = np.zeros(num_file, dtype=np.float64)\n",
    "\n",
    "for i,j in enumerate(list_all):\n",
    "    with h5py.File(file_path[j],'r') as f:      \n",
    "        gauge_length_all[i] = f['Acquisition'].attrs['GaugeLength']\n",
    "        delta_space_all[i] = f['Acquisition'].attrs['SpatialSamplingInterval']\n",
    "        sample_rate_all[i]  = f['Acquisition']['Raw[0]'].attrs['OutputDataRate']\n",
    "        num_channel_all[i] = f['Acquisition']['Raw[0]'].attrs['NumberOfLoci']\n",
    "        num_sample_all[i]  = len(f['Acquisition']['Raw[0]']['RawDataTime'][:])\n",
    "        \n",
    "# %% exclude files of which length is not 120,000\n",
    "ind_good = np.where(num_sample_all == 120000)[0]\n",
    "gauge_length_all = gauge_length_all[ind_good]\n",
    "delta_space_all = delta_space_all[ind_good]\n",
    "sample_rate_all = sample_rate_all[ind_good]\n",
    "num_channel_all = num_channel_all[ind_good]\n",
    "num_sample_all = num_sample_all[ind_good]\n",
    "list_all = list_all[ind_good]\n",
    "\n",
    "print(f'good acqusition for {len(ind_good)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c381e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %% See if the acquisition time is continuous\n",
    "file_list = file_list[list_all]\n",
    "acqu_time = acqu_time[list_all]\n",
    "file_path = [os.path.join(data_dir,i) for i in file_list]\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(13, 2.5), constrained_layout=True)\n",
    "ax.scatter(list_all, acqu_time.astype('datetime64[m]'), marker='o', s=0.1, edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce81da2",
   "metadata": {},
   "source": [
    "## 0-36 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0dc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ch, end_ch = 44, 94            # channel range\n",
    "ch_id = 33                           # choose a channel to visualize\n",
    "start_minutes = 0                    # starting file indice for reading\n",
    "num_minutes = 2160                    # number of semi-continuous 1-min files to merge\n",
    "num_seconds = int(num_minutes * 60)  # total duration (s) of merged time series\n",
    "dsamp_factor = 20                    # downsample rate when reading raw time series\n",
    "sample_rate = int(2000 / dsamp_factor)    # final sample rate after downsampling\n",
    "shift_min = 7                        # shift in minutes of the first tick when ploting\n",
    "shift_sec = int(shift_min * 60)      # shift in seconds\n",
    "file_inc = 360                       # plot ticks for every {file_inc} 1-min files \n",
    "time_inc = int(file_inc * 60)        # tick interval in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53d876",
   "metadata": {},
   "source": [
    "### The next 3 cells read the data, slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% read the raw data file\n",
    "# %% could be the most time-consuming\n",
    "# %% Do Not run this cell after the FIRST time\n",
    "# %% instead, run the next cell \n",
    "since = time.time()\n",
    "\n",
    "# %% multi-process to read and decimate lots of files \n",
    "num_proc = 20   # number of threads (too large number kills the memory) \n",
    "partial_func = partial(read_decimate, dsamp_factor=dsamp_factor, start_ch=start_ch, end_ch=end_ch)\n",
    "with Pool(processes=num_proc) as pool:   # pool is closed automatically and join as a list\n",
    "    print(\"# threads: \", num_proc)\n",
    "    full_time = pool.map(partial_func, file_path[start_minutes:start_minutes+num_minutes])\n",
    "\n",
    "# %% concatenate the list elements in time\n",
    "full_time_data = np.concatenate(full_time, axis=1)\n",
    "\n",
    "print(f'time used: {time.time()- since:.1f}')\n",
    "print(f'final shape: {full_time_data.shape}')\n",
    "print(f'sample rate: {sample_rate:.0f}')\n",
    "\n",
    "# %% save it to HDF5 for future use\n",
    "datah5 = '/fd1/QibinShi_data/England_farm/farmDAS_harper_0_24hr.hdf5'\n",
    "with h5py.File(datah5, 'w') as f:\n",
    "    f.create_dataset(\"data\", data=full_time_data)\n",
    "    f.create_dataset(\"timestamp\", data=time_stamp)\n",
    "    f.create_dataset(\"dt\", data=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Run this cell only after the FIRST time\n",
    "# %% read from the saved HDF5\n",
    "datah5 = '/fd1/QibinShi_data/England_farm/farmDAS_harper_0_24hr.hdf5'\n",
    "with h5py.File(datah5, 'r') as f:\n",
    "    full_time_data = f[\"data\"][:]\n",
    "    sample_rate = f[\"dt\"][()]\n",
    "    time_stamp = f[\"timestamp\"][:]\n",
    "acqu_time = np.array([UTCDateTime(i) for i in time_stamp])\n",
    "nsec = int(full_time_data.shape[1]/sample_rate) # total time of merged time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% plot data in time-space\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 4), constrained_layout=True)\n",
    "max_amp = np.percentile(np.fabs(full_time_data), q=80)\n",
    "cmap=plt.cm.get_cmap('RdBu')\n",
    "\n",
    "x=np.arange(full_time_data.shape[1])\n",
    "y=np.arange(full_time_data.shape[0])\n",
    "ax[0].pcolormesh(x, y, full_time_data, shading='auto', vmin=-max_amp, vmax=max_amp, cmap=cmap)\n",
    "ax[0].set_xticks(np.linspace(0,nsec*sample_rate,7))\n",
    "ax[0].set_xticklabels(acqu_time[(np.linspace(0,nsec/60,7)).astype(int)].astype('datetime64[m]'))\n",
    "ax[0].set_yticks(np.arange(0, 50, 10))\n",
    "ax[0].set_yticklabels(np.round(np.arange(0, 50, 10)*3.1904762684013206, decimals=1))\n",
    "ax[0].set_ylabel('Distance (m)'); ax[0].set_title('Full continuous data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29551cdb",
   "metadata": {},
   "source": [
    "# Cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c874e9",
   "metadata": {},
   "source": [
    "## Visualize an example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ch, end_ch = 44, 94\n",
    "with h5py.File(file_path[100],'r') as f:\n",
    "    gauge_len = f['Acquisition'].attrs['GaugeLength']\n",
    "    delta_space = f['Acquisition'].attrs['SpatialSamplingInterval']\n",
    "    sample_rate  = f['Acquisition']['Raw[0]'].attrs['OutputDataRate']\n",
    "    num_channel = f['Acquisition']['Raw[0]'].attrs['NumberOfLoci']\n",
    "    num_sample  = len(f['Acquisition']['Raw[0]']['RawDataTime'][:])\n",
    "    minute_data = f['Acquisition']['Raw[0]']['RawData'][:num_sample, start_ch:end_ch].T\n",
    "\n",
    "    delta_time = 1.0 / sample_rate\n",
    "        \n",
    "\n",
    "print(f'1-min data with shape: {minute_data.shape}')\n",
    "print('-'*12)\n",
    "print(f'Gauge length (m): {gauge_len}')\n",
    "print(f'Channel spacing (m): {delta_space}')\n",
    "print(f'Num channels: {num_channel}')\n",
    "print(f'Total cable length (m): {delta_space * num_channel}')\n",
    "print('-'*12)\n",
    "print(f'Sampling rate (Hz): {delta_time}')\n",
    "print(f'Num samples: {num_sample}')\n",
    "print(f'Total duration {delta_time * num_sample:.2f}')\n",
    "\n",
    "max_amp = np.median(np.fabs(minute_data))\n",
    "max_amp =30\n",
    "plt.figure(figsize = (10, 5), dpi = 100)\n",
    "plt.imshow(minute_data, aspect = 'auto', cmap = 'RdBu', vmax = max_amp, vmin = -max_amp, origin='lower')\n",
    "plt.ylabel(\"Distance (m)\", fontsize = 20)\n",
    "plt.xlabel(\"Time (s)\", fontsize = 20)\n",
    "plt.xticks(np.linspace(0, minute_data.shape[1], 7), \n",
    "           [i/sample_rate for i in np.linspace(0, minute_data.shape[1], 7)])\n",
    "plt.yticks(np.linspace(0, minute_data.shape[0], 6), \n",
    "           [int(i*delta_space) for i in np.linspace(0, minute_data.shape[0], 6)])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca6275",
   "metadata": {},
   "source": [
    "## Configure parameters for NoisePy processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_freq = 200                # targeted sampling rate\n",
    "freqmin   = 5                  # pre filtering frequency bandwidth\n",
    "freqmax   = 90                 # note this cannot exceed Nquist freq\n",
    "\n",
    "freq_norm   = 'phase_only'     # 'no' for no whitening, or 'rma' for running-mean average, 'phase_only' for sign-bit normalization in freq domain.\n",
    "time_norm   = 'one_bit'        # 'no' for no normalization, or 'rma', 'one_bit' for normalization in time domain\n",
    "cc_method   = 'xcorr'          # 'xcorr' for pure cross correlation, 'deconv' for deconvolution; FOR \"COHERENCY\" PLEASE set freq_norm to \"rma\", time_norm to \"no\" and cc_method to \"xcorr\"\n",
    "smooth_N    = 50               # moving window length for time domain normalization if selected (points)\n",
    "smoothspect_N  = 50            # moving window length to smooth spectrum amplitude (points)\n",
    "maxlag      = 8                # lags of cross-correlation to save (sec)\n",
    "\n",
    "# criteria for data selection\n",
    "max_over_std = 10 *9              # threahold to remove window of bad signals: set it to 10*9 if prefer not to remove them\n",
    "\n",
    "cc_len = delta_time * num_sample  # correlate length in second\n",
    "step   = delta_time * num_sample  # stepping length in second\n",
    "\n",
    "# start and end channel index for the sub-array\n",
    "cha1, cha2 = start_ch, end_ch\n",
    "\n",
    "cha_list = np.array(range(cha1, cha2)) \n",
    "nsta = len(cha_list)\n",
    "n_pair = int((nsta+1)*nsta/2)\n",
    "n_lag = maxlag * samp_freq * 2 + 1\n",
    "\n",
    "prepro_para = {'freqmin':freqmin,\n",
    "               'freqmax':freqmax,\n",
    "               'sps':sample_rate,\n",
    "               'npts_chunk':cc_len*sample_rate,\n",
    "               'nsta':nsta,\n",
    "               'cha_list':cha_list,\n",
    "               'samp_freq':samp_freq,\n",
    "               'freq_norm':freq_norm,\n",
    "               'time_norm':time_norm,\n",
    "               'cc_method':cc_method,\n",
    "               'smooth_N':smooth_N,\n",
    "               'smoothspect_N':smoothspect_N,\n",
    "               'maxlag':maxlag,\n",
    "               'max_over_std':max_over_std}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c7c73",
   "metadata": {},
   "source": [
    "## NoisePy pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_stdS, dataS = DAS_module.preprocess_raw_make_stat(minute_data.T, prepro_para)\n",
    "max_amp = np.median(np.fabs(dataS)) / 5\n",
    "plt.figure(figsize = (10, 5), dpi = 100)\n",
    "plt.imshow(dataS, aspect = 'auto', \n",
    "           cmap = 'RdBu', vmax = max_amp, vmin = -max_amp, origin='lower')\n",
    "plt.ylabel(\"Channel number\", fontsize = 20)\n",
    "plt.xlabel(\"Sample\", fontsize = 20)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_spect = DAS_module.noise_processing(dataS, prepro_para)\n",
    "Nfft = white_spect.shape[1]; Nfft2 = Nfft // 2\n",
    "data = white_spect[:, :Nfft2]\n",
    "del dataS, white_spect\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "ind = np.where((trace_stdS < prepro_para['max_over_std']) &\n",
    "                        (trace_stdS > 0) &\n",
    "                (np.isnan(trace_stdS) == 0))[0]\n",
    "if not len(ind):\n",
    "    raise ValueError('the max_over_std criteria is too high which results in no data')\n",
    "sta = cha_list[ind]\n",
    "white_spect = data[ind]\n",
    "\n",
    "print(white_spect.shape, white_spect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91519676",
   "metadata": {},
   "source": [
    "# Channel-wise correlation and stacking over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78786b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(file_path))\n",
    "start_file = np.arange(0, len(list_all)-60, 5)\n",
    "print(start_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99300ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for istack in start_file:\n",
    "    list_hour = np.arange(istack, istack + 10)\n",
    "    \n",
    "    pbar = tqdm(list_hour)\n",
    "    corr_full = np.zeros([n_lag, n_pair], dtype = np.float32)\n",
    "    stack_full = np.zeros([1, n_pair], dtype = np.int32)\n",
    "\n",
    "    for imin in pbar:\n",
    "        t0 = time.time()\n",
    "        pbar.set_description(f\"Processing {imin} th file\")\n",
    "        with h5py.File(file_path[imin],'r') as f:\n",
    "            gauge_len = f['Acquisition'].attrs['GaugeLength']\n",
    "            delta_space = f['Acquisition'].attrs['SpatialSamplingInterval']\n",
    "            sample_rate  = f['Acquisition']['Raw[0]'].attrs['OutputDataRate']\n",
    "            num_sample  = len(f['Acquisition']['Raw[0]']['RawDataTime'][:])\n",
    "            minute_data = f['Acquisition']['Raw[0]']['RawData'][:num_sample, start_ch:end_ch]\n",
    "\n",
    "\n",
    "        # perform pre-processing\n",
    "        trace_stdS, dataS = DAS_module.preprocess_raw_make_stat(minute_data, prepro_para)\n",
    "\n",
    "        # do normalization if needed\n",
    "        white_spect = DAS_module.noise_processing(dataS, prepro_para)\n",
    "        Nfft = white_spect.shape[1]; Nfft2 = Nfft // 2\n",
    "        data = white_spect[:, :Nfft2]\n",
    "        del dataS, white_spect\n",
    "\n",
    "        ind = np.where((trace_stdS < prepro_para['max_over_std']) &\n",
    "                                (trace_stdS > 0) &\n",
    "                        (np.isnan(trace_stdS) == 0))[0]\n",
    "        if not len(ind):\n",
    "            raise ValueError('the max_over_std criteria is too high which results in no data')\n",
    "        sta = cha_list[ind]\n",
    "        white_spect = data[ind]\n",
    "\n",
    "        # loop over all stations\n",
    "        for iiS in range(len(sta)):\n",
    "            # smooth the source spectrum\n",
    "            sfft1 = DAS_module.smooth_source_spect(white_spect[iiS], prepro_para)\n",
    "\n",
    "            # correlate one source with all receivers\n",
    "            corr, tindx = DAS_module.correlate(sfft1, white_spect[iiS:], prepro_para, Nfft)\n",
    "            print(f\"source channel index: {sta[iiS]}, correlation shape: {corr.shape}\")\n",
    "\n",
    "            # update the receiver list\n",
    "            tsta = sta[iiS:]\n",
    "            receiver_lst = tsta[tindx]\n",
    "\n",
    "            iS = int((cha2*2 - cha1 - sta[iiS] + 1) * (sta[iiS] - cha1) / 2)\n",
    "\n",
    "            # stacking one minute\n",
    "            corr_full[:, iS + receiver_lst - sta[iiS]] += corr.T\n",
    "            stack_full[:, iS + receiver_lst - sta[iiS]] += 1\n",
    "\n",
    "    corr_full /= stack_full\n",
    "    \n",
    "    plt.figure(figsize = (12, 5), dpi = 200)\n",
    "#     max_amp = np.median(np.fabs(corr_full[:, :(cha2 - cha1)])) * 10\n",
    "    max_amp = 0.004\n",
    "    plt.imshow(corr_full[1200:2000, :(cha2 - cha1)].T, aspect = 'auto', cmap = 'RdBu', \n",
    "               vmax = max_amp, vmin = -max_amp, origin = 'lower')\n",
    "\n",
    "\n",
    "    plt.yticks(np.linspace(cha1, cha2, 4) - cha1, \n",
    "                  [int(i*delta_space) for i in np.linspace(cha1, cha2, 4)], fontsize = 12)\n",
    "    plt.xticks(np.arange(0, 800, 200), (np.arange(-400, 400, 200))/samp_freq, fontsize = 12)\n",
    "\n",
    "    # linex=np.arange(400, 480, 10)\n",
    "    # liney=((linex-400)/samp_freq*400) / delta_space\n",
    "    # plt.plot(linex,liney,color='white', linestyle='dashed',linewidth=2)\n",
    "\n",
    "    plt.axvline(x = 400, color = 'k')\n",
    "\n",
    "    plt.ylabel(\"Distance (m)\", fontsize = 16)\n",
    "    plt.xlabel(\"Time lag (sec)\", fontsize = 16)\n",
    "    plt.title(str(acqu_time[istack]), fontsize = 20)\n",
    "    bar = plt.colorbar(format = lambda x, pos: '{:}'.format(x*100))\n",
    "    bar.set_label('Cross-correlation Coefficient ($\\\\times10^{-2}$)', fontsize = 15)\n",
    "\n",
    "#     plt.savefig('/data/whd01/qibin_data/farm_hour_stack_png/'+str(istack).zfill(4)+ '.pdf', dpi=150, format='pdf', bbox_inches='tight')\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206290a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
